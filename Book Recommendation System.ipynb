{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Book Recommendation System\n",
    "\n",
    "In this notebook, we will build a book recommendation system based on all the books articles on Wikipedia and neural network embeddings of books and links. This is a great look at the potential for neural networks to create meaningful embeddings of high dimensional data and one practical application of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "The data is stored as json with line for every book. This data contains every single book article on Wikipedia which was parsed in the Downloading and Parsing Wikipedia Data Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "c = ConfigManager()\n",
    "c.update('notebook', {\"CodeCell\": {\"cm_config\": {\"autoCloseBrackets\": False}}})\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37040 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('found_books_filtered.ndjson', 'r') as fin:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(l) for l in fin]\n",
    "\n",
    "\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each book contains the title, the information from the `Infobox book` template, the internal wikipedia links, the external links, the date of last edit, and the number of characters in the article (a rough estimate of the length of the article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Derech Mitzvosecha',\n",
       " {'name': 'Derech Mitzvosecha (Sefer Hamitzvos)',\n",
       "  'image': 'Derech Mitzvosecha 2014-05-08 01-34.jpg',\n",
       "  'image_size': '200px',\n",
       "  'caption': 'Derech Mitzvosecha, 1912 edition',\n",
       "  'author': 'Rabbi Menachem Mendel Schneersohn ( \" Tzemach Tzedek \" ), the third Rebbe of Chabad',\n",
       "  'genre': 'non-fiction',\n",
       "  'isbn': '0826655904',\n",
       "  'published': '7x10 Hardcover, 570pp, (Kehot Publication Society, Brooklyn New York)',\n",
       "  'subject': 'Jewish mysticism, Chabad philosophy',\n",
       "  'language': 'Hebrew'},\n",
       " ['Mitzvos',\n",
       "  'Menachem Mendel Schneersohn',\n",
       "  'Rebbe',\n",
       "  'Chabad',\n",
       "  'Hasidic',\n",
       "  'Chabad philosophy',\n",
       "  'Tzitzit',\n",
       "  'Tefillin',\n",
       "  'Hillel the Elder',\n",
       "  'Poltava',\n",
       "  'Kehot Publication Society',\n",
       "  'Eliyahu Touger',\n",
       "  'Sichos in English',\n",
       "  'Tanya',\n",
       "  'Shneur Zalman of Liadi',\n",
       "  'Category:Chabad-Lubavitch (Hasidic dynasty)',\n",
       "  'Category:Books about Judaism',\n",
       "  'Category:Chabad-Lubavitch texts'],\n",
       " ['http://www.books.google.com/books?id=HYLi3l9ylWwC',\n",
       "  'http://www.store.kehotonline.com/index.php?stocknumber=HTZ-DEREM',\n",
       "  'http://www.shturem.org/index.php?section=news',\n",
       "  'http://www.oxfordchabad.org/templates/blog/post_cdo/AID/708481/PostID/36991',\n",
       "  'http://www.chabadlibrary.org/exhibit/ex4/exed5.htm',\n",
       "  'http://www.hebrewbooks.org/15419',\n",
       "  'http://hebrewbooks.org/16082',\n",
       "  'http://www.hebrewbooks.org/15419'],\n",
       " '2018-02-01T01:15:03Z',\n",
       " 4957]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Books to Integers\n",
    "\n",
    "First we want to create a mapping of books to integers. When we feed books into the neural network, we will have to represent them as numbers, so this mapping will let us keep track of the books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22475"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'War and Peace'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['War and Peace']\n",
    "index_book[22475]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Unique Wikilinks\n",
    "\n",
    "First we can do a little data exploration. Let's find the number of unique Wikilinks (links to other Wikipedia articles) and the most common ones. To create a single list from a list of lists, we can use the `itertools` chain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 315703 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "wikilinks = list(chain(*[book[2] for book in books]))\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these are links to other books? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17036 unique links to other books.\n"
     ]
    }
   ],
   "source": [
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique links to other books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the counts of each wikilink. We'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7643),\n",
       " ('Paperback', 7484),\n",
       " ('Wikipedia:WikiProject Books', 6116),\n",
       " ('Wikipedia:WikiProject Novels', 6087),\n",
       " ('English language', 4308),\n",
       " ('The New York Times', 3960),\n",
       " ('United States', 3394),\n",
       " ('Science fiction', 3121),\n",
       " ('science fiction', 2661),\n",
       " ('Publishers Weekly', 2415)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most linked to pages are in fact not that surprising! One thing we should notice is that there are discrepancies in capitalization. We want to normalize across capitalization, so we'll lowercase all of the links and redo the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 301955 unique wikilinks.\n"
     ]
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paperback', 8979),\n",
       " ('hardcover', 8856),\n",
       " ('wikipedia:wikiproject books', 6116),\n",
       " ('wikipedia:wikiproject novels', 6088),\n",
       " ('science fiction', 5917),\n",
       " ('english language', 4375),\n",
       " ('the new york times', 3976),\n",
       " ('united states', 3397),\n",
       " ('novel', 3045),\n",
       " ('publishers weekly', 2415)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That actually changes the rankings! This illustrates an important point: make sure to take a look at your data before modeling! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the discontinuity guide', 148),\n",
       " ('the encyclopedia of science fiction', 145),\n",
       " ('dracula', 72),\n",
       " ('the dresden files', 70),\n",
       " ('the encyclopedia of fantasy', 66),\n",
       " ('encyclopÃ¦dia britannica', 61),\n",
       " ('the wonderful wizard of oz', 61),\n",
       " ('nineteen eighty-four', 59),\n",
       " (\"alice's adventures in wonderland\", 54),\n",
       " ('don quixote', 52)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks_other_books = [link.lower() for link in wikilinks_other_books]\n",
    "wikilink_books_counts = count_items(wikilinks_other_books)\n",
    "list(wikilink_books_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many unique wikilinks, I'm going to limit the list to wikilinks mentioned 5 or more times. Hopefully this reduces the noise that might come from wikilinks that only appear a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['catherine ryan hyde',\n",
       " 'life with billy (book)',\n",
       " 'shan states',\n",
       " 'burmese chronicles#ramanya',\n",
       " 'northern ndebele people',\n",
       " 'international journal of hindu studies',\n",
       " 'charles henderson (character)',\n",
       " 'anarchy: a journal of desire armed',\n",
       " 'american museum of fly fishing',\n",
       " 'key of perihelion']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 5]\n",
    "print(len(links))\n",
    "links[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pound sign in the link name means that the link directs to a specific section on the Wikipedia page. Although we could normalize these to the title of the wikipedia article, we'll leave the pound sign in for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikilinks to Index\n",
    "\n",
    "As with the books, we need to map the wikilinks to integers. We'll also create the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the new york times'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the new york times']\n",
    "index_link[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Training Set\n",
    "\n",
    "In order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given an article, can the neural network determine whether or not a wikilink was present on that page? To do this, we need to create a dictionary of books and the associate links on their page. We already have this in the list of books, but now we need to change our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37000\r"
     ]
    }
   ],
   "source": [
    "# book_link_mapping = {book_index[book[0]]: [link_index[link.lower()] for link in book[2] if link.lower() in links] for book in books}\n",
    "\n",
    "# book_link_mapping = {}\n",
    "\n",
    "# for i, book in enumerate(books):\n",
    "#     book_link_mapping[book_index[book[0]]] = []\n",
    "#     for link in book[2]:\n",
    "#         if link.lower() in links:\n",
    "#             book_link_mapping[book_index[book[0]]].append(link_index[link.lower()])\n",
    "    \n",
    "#     if i % 100 == 0:\n",
    "#         print(i, end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each book and each wikilink in the books articles, we'll add it to a list of pairs with the form (book_index, link_index). We also filter out the links that did not have at least 5 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(805304, 38935, 37040)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "\n",
    "for book in books:\n",
    "    pairs.extend((book_index[book[0]], link_index[link.lower()]) for link in book[2] if link.lower() in links)\n",
    "    \n",
    "len(pairs), len(links), len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 805304 positive examples on which to train! Each pair represents one internal wikipedia link for one book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Doraemon: Nobita's Space Heroes\", 'south korea')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[5000][0]], index_link[pairs[5000][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('KP: The Autobiography', 'hardcover')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[10000][0]], index_link[pairs[10000][1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more common wikilinks might be less useful. In a way, this could be similar to problem that creates a need for TF-IDF: the more common a word, the less representative it is of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator For Training Samples\n",
    "\n",
    "We want to generate positive samples and negative samples. The positive samples are simple: pick one random pair from the `pairs` and assign it a 1. The negative samples are also fairly easy: pick one random link and one random book, make sure they are not in the `pairs`, and assign them a -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0):\n",
    "    \"\"\"Generate batches of samples for training\"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    \n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    while True:\n",
    "        # randomly choose positive examples\n",
    "        for idx, (book_id, link_id) in enumerate(random.sample(pairs, n_positive)):\n",
    "            batch[idx, :] = (book_id, link_id, 1)\n",
    "            \n",
    "        idx = n_positive\n",
    "        \n",
    "        # Add negative examples until reach batch size\n",
    "        while idx < batch_size:\n",
    "            \n",
    "            # random selection\n",
    "            random_book = random.randrange(len(books))\n",
    "            random_link = random.randrange(len(links))\n",
    "            \n",
    "            # Check to make sure this is not a positive example\n",
    "            if (random_book, random_link) not in pairs:\n",
    "                batch[idx, :] = (random_book, random_link, -1)\n",
    "                idx += 1\n",
    "                \n",
    "        np.random.shuffle(batch)\n",
    "        yield {'book': batch[:, 0], 'link': batch[:, 1]}, batch[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'book': array([21977., 29880., 20277.,  5792.,  3138.,  3812.]),\n",
       "  'link': array([15053., 17263., 13360., 12904., 13401., 28269.])},\n",
       " array([-1., -1., -1.,  1., -1.,  1.]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(generate_batch(pairs, n_positive = 2, negative_ratio = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Book Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dot, Reshape\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "book (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "link (InputLayer)               (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 50)        1852000     book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "link_embedding (Embedding)      (None, 1, 50)        1946750     link[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 50)           0           book_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, 50)           0           link_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_product (Dot)               (None, 1)            0           reshape_3[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,798,750\n",
      "Trainable params: 3,798,750\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def book_embedding_model(embedding_size = 50):\n",
    "    book = Input(name = 'book', shape = [1])\n",
    "    link = Input(name = 'link', shape = [1])\n",
    "    \n",
    "    book_embedding = Embedding(name = 'book_embedding',\n",
    "                               input_dim = len(book_index),\n",
    "                               output_dim = embedding_size)(book)\n",
    "    \n",
    "    link_embedding = Embedding(name = 'link_embedding',\n",
    "                               input_dim = len(link_index),\n",
    "                               output_dim = embedding_size)(link)\n",
    "    \n",
    "    book_embedding = Reshape(target_shape = [embedding_size])(book_embedding)\n",
    "    link_embedding = Reshape(target_shape = [embedding_size])(link_embedding)\n",
    "    \n",
    "    merged = Dot(name = 'dot_product', normalize = True, axes = 1)([book_embedding, link_embedding])\n",
    "    model = Model(inputs = [book, link], outputs = merged)\n",
    "    model.compile(optimizer = 'Adam', loss = 'mse')\n",
    "    return model\n",
    "\n",
    "model = book_embedding_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "The next step is just to train the model. We can fit the model on the generator and also make a generator for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "train_gen = generate_batch(pairs, n_positive=512, negative_ratio=5)\n",
    "valid_gen = generate_batch(pairs, n_positive=512, negative_ratio=5)\n",
    "\n",
    "model.fit_generator(train_gen, epochs = 20, \n",
    "                    steps_per_epoch = len(pairs) // 512,\n",
    "                    validation_data = valid_gen, \n",
    "                    validation_steps = len(pairs) // 4,\n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Of course the fun part is making predictions with the model. We can feed it any book and get the closest and furthest away books in the embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
